{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MastersDeliverable.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ3lEb6W37f4",
        "cellView": "both",
        "outputId": "221ad85e-34e9-4343-b084-6e786ebbee79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Mount Drive { form-width: \"30%\" }\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') ## Be sure to pick the right account\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJrVz5D3QRLJ",
        "cellView": "form"
      },
      "source": [
        "#@title Install & Import Packages { form-width: \"30%\" }\n",
        "#!pip3 install pillow\n",
        "#from PIL import Image\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.DEBUG)\n",
        "\n",
        "#!pip install --upgrade smqtk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYSoHyVqRv2_",
        "cellView": "form"
      },
      "source": [
        "#@title Setup CNN { form-width: \"30%\" }\n",
        "# https://engmrk.com/alexnet-implementation-using-keras/\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Flatten, Dropout\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import array_to_img\n",
        "from keras.models import Model\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "\n",
        "def getModelAlexNet():\n",
        "  model = Sequential()\n",
        "\n",
        "  # 1st Convolutional Layer\n",
        "  model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
        "  model.add(Activation('relu'))\n",
        "  # Max Pooling\n",
        "  model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "\n",
        "  # 2nd Convolutional Layer\n",
        "  model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
        "  model.add(Activation('relu'))\n",
        "  # Max Pooling\n",
        "  model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "\n",
        "  # 3rd Convolutional Layer\n",
        "  model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  # 4th Convolutional Layer\n",
        "  model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  # 5th Convolutional Layer\n",
        "  model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "  model.add(Activation('relu'))\n",
        "  # Max Pooling\n",
        "  model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "\n",
        "  # Passing it to a Fully Connected layer\n",
        "  model.add(Flatten())\n",
        "  # 1st Fully Connected Layer\n",
        "  model.add(Dense(4096, input_shape=(224*224*3,)))\n",
        "  model.add(Activation('relu'))\n",
        "  # Add Dropout to prevent overfitting\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  # 2nd Fully Connected Layer\n",
        "  model.add(Dense(4096))\n",
        "  model.add(Activation('relu'))\n",
        "  # Add Dropout\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  # 3rd Fully Connected Layer\n",
        "  model.add(Dense(1000))\n",
        "  model.add(Activation('relu'))\n",
        "  # Add Dropout\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  # Output Layer\n",
        "  model.add(Dense(17))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  #model.summary()\n",
        "\n",
        "  # Compile the model and load weights\n",
        "  model.compile(loss=categorical_crossentropy, optimizer='adam', metrics=[\"accuracy\"])\n",
        "  model.load_weights('/content/drive/My Drive/Mestrado/alexnet_weights.h5', by_name=True)\n",
        "\n",
        "  return model\n",
        "\n",
        "def getFeatureAlexNet(layer, images):\n",
        "  model = getModelAlexNet();\n",
        "  # redefine model to output right after the first hidden layer\n",
        "  # (like creating a new model just for doing that as a prediction)\n",
        "  model = Model(inputs=model.inputs, outputs=model.layers[layer].output)\n",
        "\n",
        "  # prepare the image (e.g. scale pixel values for the alexnet) !!!\n",
        "  #img = preprocess_input(images)\n",
        "  # get feature map for first hidden layer\n",
        "  features = model.predict(images)\n",
        "\n",
        "  logger.debug('[getFeatureAlexNet] out: ' + str(features.shape))\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byNC2YgvhbXB",
        "cellView": "form"
      },
      "source": [
        "#@title Load Input Frames { form-width: \"30%\" }\n",
        "\n",
        "def loadInput():\n",
        "  # load the image with the required shape (in this case its 224,224 for alexnet)\n",
        "  img = load_img('/content/drive/My Drive/Mestrado/UCSDped2/Test/Test012/180.tif', target_size=(224, 224)) #240,360 #224,224\n",
        "  # convert the image to an array\n",
        "  img = img_to_array(img)\n",
        "  # expand dimensions so that it represents a single 'sample'\n",
        "  img = expand_dims(img, axis=0)\n",
        "\n",
        "  logger.debug('[loadInput] out: ' + str(img.shape))\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1ZsnXscQp82",
        "cellView": "form",
        "outputId": "57fd3a60-7988-48e8-b9e2-e67857a354bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#@title Extract Feature Maps { form-width: \"30%\" }\n",
        "\n",
        "img = loadInput()\n",
        "frames = img.shape[0]\n",
        "\n",
        "## The first run of this will always fail ##\n",
        "feats = getFeatureAlexNet(10, img) # 20 is fc8, 17 is fc7, 10 is conv6\n",
        "print(img.shape)\n",
        "print(feats.shape)\n",
        "rowsFeats = (feats[0].shape)[0]\n",
        "colsFeats = (feats[0][0].shape)[0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 224, 224, 3)\n",
            "(1, 2, 2, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXqHAGcnkC2H",
        "cellView": "form"
      },
      "source": [
        "#@title ITQ { form-width: \"30%\" }\n",
        "from collections import Sequence\n",
        "import sys\n",
        "import numpy as np\n",
        "import pdb\n",
        "# from smqtk.representation.descriptor_element._io import elements_to_matrix\n",
        "\n",
        "\n",
        "def _norm_vector(v):\n",
        "    n = np.linalg.norm(v, 2, v.ndim - 1, keepdims=True)\n",
        "    # replace 0's with 1's, preventing div-by-zero\n",
        "    n[n == 0.] = 1.\n",
        "\n",
        "    logger.debug('[_norm_vector] out: ' + str((v / n).shape))\n",
        "    return v / n\n",
        "    # Normalization off\n",
        "    #return v\n",
        "\n",
        "# Fit the ITQ model given the input set of descriptors. (compress ITQ)\n",
        "def prepareITQ(x, bit_length):\n",
        "  # logger.info(\"Creating matrix of descriptors for fitting\")\n",
        "  # x = elements_to_matrix(descriptors)\n",
        "  logger.info(\"descriptor matrix shape: %s\", x.shape)\n",
        "\n",
        "  logger.info(\"Info normalizing descriptors by factor: fro\")\n",
        "  x = _norm_vector(x)\n",
        "  norm_x = x\n",
        "\n",
        "  ## compress ITQ starts here\n",
        "  logger.info(\"Centering data\")\n",
        "  mean_vec = np.mean(x, axis=0)\n",
        "  x -= mean_vec # (X - repmat(mean_vec,size(X,1),1))\n",
        "\n",
        "  logger.info(\"Computing PCA transformation\")\n",
        "  logger.info(\"-- computing covariance\")\n",
        "  # ``cov`` wants each row to be a feature and each column an observation\n",
        "  # of those features. Thus, each column should be a descriptor vector,\n",
        "  # thus we need the transpose here.\n",
        "  c = np.cov(x.transpose())\n",
        "  # c = np.cov(x)\n",
        "\n",
        "  # Direct translation from UNC matlab code\n",
        "  # - eigen vectors are the columns of ``pc``\n",
        "  logger.info('-- computing linalg.eig')\n",
        "  l, pc = np.linalg.eig(c)\n",
        "  # [pc, l] = eigs(C, bit)\n",
        "  logger.info('-- ordering eigen vectors by descending eigen '\n",
        "                  'value')\n",
        "\n",
        "  # # Harry translation of original matlab code\n",
        "  # # - Uses singular values / vectors, not eigen\n",
        "  # # - singular vectors are the columns of pc\n",
        "  # logging.info('-- computing linalg.svd')\n",
        "  # pc, l, _ = numpy.linalg.svd(c)\n",
        "  # logging.info('-- ordering singular vectors by descending '\n",
        "  #                 'singular value')\n",
        "\n",
        "  # Same ordering method for both eig/svd sources.\n",
        "  l_pc_ordered = sorted(zip(l, pc.transpose()), key=lambda _p: _p[0], reverse=True)\n",
        "\n",
        "  logger.info(\"-- top vector extraction\")\n",
        "  # Only keep the top ``bit_length`` vectors after ordering by descending\n",
        "  # value magnitude.\n",
        "  # - Transposing vectors back to column-vectors.\n",
        "  pc_top = np.array([p[1] for p in l_pc_ordered[:bit_length]]).transpose()\n",
        "  logger.info(\"-- project centered data by PC matrix\")\n",
        "  v = np.dot(x, pc_top) # XX = X*pc;\n",
        "\n",
        "  logger.info(\"Performing ITQ to find optimal rotation\")\n",
        "  c, r = ITQ(v, 50) # MattRb code uses 300 or 100\n",
        "  # De-adjust rotation with PC vector\n",
        "  r = np.dot(pc_top, r) #project_mat = pca_mapping * itq_rot_mat ?\n",
        "\n",
        "  #self.save_model() adds the matrices to a cache in the object\n",
        "\n",
        "  return c, pc, r, norm_x\n",
        "\n",
        "# main function for ITQ which finds a rotation of the PCA embedded data\n",
        "# Input:\n",
        "#       V: n*c PCA embedded data, n is the number of images and c is the\n",
        "#       code length\n",
        "#       n_iter: max number of iterations, 50 is usually enough\n",
        "# Output:\n",
        "#       B: n*c binary matrix\n",
        "#       R: the c*c rotation matrix found by ITQ\n",
        "\n",
        "def ITQ(v, n_iter):\n",
        "  # python version can be found on  _find_itq_rotation\n",
        "  # initialize with a orthogonal random rotation\n",
        "  bit = np.size(v,1); # size of dimension 2 (or v.shape[1] \"Pull num bits from PCA-projected descriptors\")\n",
        "  r = np.random.randn(bit,bit);\n",
        "  u11, s2, v2 = np.linalg.svd(r);\n",
        "  #R = U11(:,1:bit); #entire row, first through bit elements inclusive\n",
        "  r = u11[:, :bit]\n",
        "  #r = u11[:,0:bit+1]?\n",
        "\n",
        "  # ITQ to find optimal rotation\n",
        "  logger.info(\"ITQ iterations to determine optimal rotation: %d\", n_iter)\n",
        "  for i in range(n_iter):\n",
        "      z = np.dot(v, r)      \n",
        "      ux = np.ones(z.shape)*(-1)\n",
        "      ux[z >= 0] = 1\n",
        "      c = np.dot(ux.transpose(), v)\n",
        "      ub, sigma, ua = np.linalg.svd(c)\n",
        "      r = np.dot(ua, ub.transpose())\n",
        "\n",
        "  # make B binary\n",
        "\n",
        "  #z = np.dot(v, r) recalculate z so it doesnt get desync\n",
        "  #b = np.zeros(z.shape, dtype=np.bool)\n",
        "  #b[z >= 0] = True\n",
        "  b = ux # original\n",
        "  b[b < 0] = 0 # original\n",
        "\n",
        "  logger.debug('[ITQ] out: ' + str(b.shape) + ',' + str(r.shape))\n",
        "  return b,r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgh_iAc3Q1UW",
        "cellView": "both",
        "outputId": "bb991c39-f65f-4596-fe41-ce90213cf7f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#@title Convert to Binary Maps { form-width: \"30%\" }\n",
        "#logging.basicConfig(stream=sys.stderr, level=logging.ERROR)\n",
        "\n",
        "def featureVectorToBinVector(feats):\n",
        "  array = []\n",
        "  for image in feats:\n",
        "    for row in image:\n",
        "      for col in row:\n",
        "        array.append(col)\n",
        "  array = np.array(array)\n",
        "\n",
        "  c, pc, r, norm_x = prepareITQ(array, 7);\n",
        "  logger.debug('[featureVectorToBinVector] out: ' + str(c.shape))\n",
        "  return c\n",
        "\n",
        "binFeats = []\n",
        "for frame in range(frames):\n",
        "  frameFeats = featureVectorToBinVector(feats)\n",
        "  frameFeats = frameFeats.reshape(rowsFeats, colsFeats, 7)\n",
        "  print(frameFeats.shape)\n",
        "  binFeats.append(frameFeats)\n",
        "\n",
        "binFeats = np.array(binFeats)\n",
        "print(binFeats.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 2, 7)\n",
            "(1, 2, 2, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVt-uxKPSc9a",
        "outputId": "6db307a2-000b-4018-ca49-5f891e9c6fb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# plot all 4 vectors in an 2x2 squares\n",
        "width = 2\n",
        "height = 2\n",
        "depth = 7\n",
        "ix = 1\n",
        "for _ in range(height):\n",
        "\tfor _ in range(width):\n",
        "\t\tfor _ in range(depth):\n",
        "\t\t\t# specify subplot and turn of axis      \n",
        "\t\t\tprint(binFeats[0, :, :, :])\n",
        "\t\tix += 1\n",
        "# show the figure\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n",
            "[[[0. 1. 0. 1. 0. 0. 1.]\n",
            "  [1. 1. 1. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0. 0. 1. 1. 1.]\n",
            "  [1. 0. 0. 1. 1. 1. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}